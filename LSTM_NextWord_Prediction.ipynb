{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\" About the Program\n",
        "What is the fee for the Full Stack Web Development Bootcamp?\n",
        "The bootcamp follows a monthly subscription model where you pay Rs 999/month.\n",
        "What is the total duration of the bootcamp?\n",
        "The total duration of the bootcamp is 6 months, so the total fee becomes 999*6 = Rs 6000 (approx).\n",
        "What is the syllabus of the bootcamp?\n",
        "We cover the following modules:\n",
        "HTML & CSS\n",
        "JavaScript (Basic to Advanced)\n",
        "Version Control with Git & GitHub\n",
        "React.js\n",
        "Node.js & Express\n",
        "MongoDB & SQL\n",
        "REST APIs\n",
        "Authentication & Authorization\n",
        "Deployment using Vercel & AWS\n",
        "You can check the detailed syllabus here - https://example.com/fullstack-syllabus\n",
        "Will DevOps or CI/CD be covered in this bootcamp?\n",
        "Only introductory topics will be covered; the main focus is on MERN stack development.\n",
        "What if I miss a live session? Will I get a recording?\n",
        "Yes, all sessions are recorded and uploaded to your dashboard.\n",
        "Where can I find the class schedule?\n",
        "Check the official schedule sheet here - https://example.com/schedule\n",
        "What is the average duration of a live session?\n",
        "Approximately 2.5 hours per session.\n",
        "What is the mode of instruction?\n",
        "Hinglish (mix of Hindi and English)\n",
        "How will I be informed about the upcoming class?\n",
        "You will get an email and WhatsApp reminder before each session.\n",
        "Can I join the course if I have no technical background?\n",
        "Yes, the bootcamp is beginner-friendly.\n",
        "Is it possible to join in the middle of the bootcamp?\n",
        "Absolutely. Once you join, all past sessions will be unlocked.\n",
        "Do I need to submit assignments?\n",
        "No need to submit. Solutions will be provided for self-evaluation.\n",
        "Will we build real projects in the course?\n",
        "Yes, each module includes a capstone project.\n",
        "Where can I contact for general support?\n",
        "You can write to support@webcamp.in\n",
        "Payment/Registration Questions\n",
        "Where should I make my payments?\n",
        "All payments must be done on our official website - https://example.com\n",
        "Can I pay the full fee in one go?\n",
        "No, the bootcamp follows a monthly subscription model only.\n",
        "What is the validity of each monthly payment?\n",
        "30 days from the date of payment.\n",
        "Is there a refund policy?\n",
        "Yes, we offer a 5-day refund policy from the day of payment.\n",
        "I live outside India and can’t make a payment, what should I do?\n",
        "Please write to us at support@webcamp.in for international payment options.\n",
        "Post Registration Queries\n",
        "How long will I have access to paid content?\n",
        "Till your subscription is active. After completing all 6 payments, access is extended till Dec 2025.\n",
        "Why is lifetime access not provided?\n",
        "Due to the low course fee, we can’t offer lifetime access.\n",
        "How can I clear doubts?\n",
        "Submit the doubt form available in your dashboard to schedule a 1-on-1 session.\n",
        "Can I ask doubts from earlier sessions if I join late?\n",
        "Yes, just select the correct session/week in the doubt form.\n",
        "Certificate & Placement Assistance\n",
        "What is the criteria to get the certificate?\n",
        "1. Complete payment (Rs 6000 total)\n",
        "2. Attempt at least 80% of the assignments\n",
        "What does placement assistance include?\n",
        "Portfolio building\n",
        "Mock interviews\n",
        "Resume review\n",
        "Referral network access\n",
        "Job hunting strategy sessions\n",
        "Does it guarantee a job or interview?\n",
        "No, we do not guarantee placements or interviews. Assistance is provided, not assurance.\"\"\""
      ],
      "metadata": {
        "id": "JKtGzgkMY7Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic6Ypf9pZerQ",
        "outputId": "18d4ed7a-1e26-4287-d603-01a0e179451f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Tokenizer\n",
            "  Downloading tokenizer-3.4.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizer-3.4.5-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Tokenizer\n",
            "Successfully installed Tokenizer-3.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnijRghnSML-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokanizer = Tokenizer()"
      ],
      "metadata": {
        "id": "iC1XGRI1YRM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokanizer.fit_on_texts([data])"
      ],
      "metadata": {
        "id": "fbXP-CWPZ9Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokanizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nzu_kJjaAqx",
        "outputId": "590efbc7-6a09-4fd6-e053-4f74d3642905",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'is': 2,\n",
              " 'i': 3,\n",
              " 'a': 4,\n",
              " 'of': 5,\n",
              " 'to': 6,\n",
              " 'what': 7,\n",
              " 'bootcamp': 8,\n",
              " 'will': 9,\n",
              " 'can': 10,\n",
              " 'in': 11,\n",
              " 'payment': 12,\n",
              " 'be': 13,\n",
              " 'session': 14,\n",
              " 'you': 15,\n",
              " 'we': 16,\n",
              " 'yes': 17,\n",
              " 'access': 18,\n",
              " 'fee': 19,\n",
              " 'for': 20,\n",
              " 'where': 21,\n",
              " 'total': 22,\n",
              " 'all': 23,\n",
              " 'sessions': 24,\n",
              " 'and': 25,\n",
              " 'schedule': 26,\n",
              " 'join': 27,\n",
              " 'no': 28,\n",
              " 'monthly': 29,\n",
              " 'subscription': 30,\n",
              " 'rs': 31,\n",
              " 'duration': 32,\n",
              " '6': 33,\n",
              " 'syllabus': 34,\n",
              " 'https': 35,\n",
              " 'example': 36,\n",
              " 'com': 37,\n",
              " 'or': 38,\n",
              " 'on': 39,\n",
              " 'if': 40,\n",
              " 'live': 41,\n",
              " 'get': 42,\n",
              " 'your': 43,\n",
              " 'how': 44,\n",
              " 'each': 45,\n",
              " 'course': 46,\n",
              " 'do': 47,\n",
              " 'submit': 48,\n",
              " 'provided': 49,\n",
              " 'support': 50,\n",
              " 'payments': 51,\n",
              " 'from': 52,\n",
              " 'not': 53,\n",
              " '1': 54,\n",
              " 'assistance': 55,\n",
              " 'about': 56,\n",
              " 'full': 57,\n",
              " 'stack': 58,\n",
              " 'development': 59,\n",
              " 'follows': 60,\n",
              " 'model': 61,\n",
              " 'pay': 62,\n",
              " '999': 63,\n",
              " '6000': 64,\n",
              " 'js': 65,\n",
              " 'check': 66,\n",
              " 'here': 67,\n",
              " 'covered': 68,\n",
              " 'only': 69,\n",
              " 'dashboard': 70,\n",
              " 'class': 71,\n",
              " 'official': 72,\n",
              " '2': 73,\n",
              " '5': 74,\n",
              " 'have': 75,\n",
              " 'it': 76,\n",
              " 'need': 77,\n",
              " 'assignments': 78,\n",
              " 'write': 79,\n",
              " 'webcamp': 80,\n",
              " 'registration': 81,\n",
              " 'should': 82,\n",
              " 'make': 83,\n",
              " 'refund': 84,\n",
              " 'policy': 85,\n",
              " 'offer': 86,\n",
              " 'day': 87,\n",
              " 'can’t': 88,\n",
              " 'at': 89,\n",
              " 'till': 90,\n",
              " 'lifetime': 91,\n",
              " 'doubts': 92,\n",
              " 'doubt': 93,\n",
              " 'form': 94,\n",
              " 'certificate': 95,\n",
              " 'placement': 96,\n",
              " 'does': 97,\n",
              " 'interviews': 98,\n",
              " 'job': 99,\n",
              " 'guarantee': 100,\n",
              " 'program': 101,\n",
              " 'web': 102,\n",
              " 'month': 103,\n",
              " 'months': 104,\n",
              " 'so': 105,\n",
              " 'becomes': 106,\n",
              " 'approx': 107,\n",
              " 'cover': 108,\n",
              " 'following': 109,\n",
              " 'modules': 110,\n",
              " 'html': 111,\n",
              " 'css': 112,\n",
              " 'javascript': 113,\n",
              " 'basic': 114,\n",
              " 'advanced': 115,\n",
              " 'version': 116,\n",
              " 'control': 117,\n",
              " 'with': 118,\n",
              " 'git': 119,\n",
              " 'github': 120,\n",
              " 'react': 121,\n",
              " 'node': 122,\n",
              " 'express': 123,\n",
              " 'mongodb': 124,\n",
              " 'sql': 125,\n",
              " 'rest': 126,\n",
              " 'apis': 127,\n",
              " 'authentication': 128,\n",
              " 'authorization': 129,\n",
              " 'deployment': 130,\n",
              " 'using': 131,\n",
              " 'vercel': 132,\n",
              " 'aws': 133,\n",
              " 'detailed': 134,\n",
              " 'fullstack': 135,\n",
              " 'devops': 136,\n",
              " 'ci': 137,\n",
              " 'cd': 138,\n",
              " 'this': 139,\n",
              " 'introductory': 140,\n",
              " 'topics': 141,\n",
              " 'main': 142,\n",
              " 'focus': 143,\n",
              " 'mern': 144,\n",
              " 'miss': 145,\n",
              " 'recording': 146,\n",
              " 'are': 147,\n",
              " 'recorded': 148,\n",
              " 'uploaded': 149,\n",
              " 'find': 150,\n",
              " 'sheet': 151,\n",
              " 'average': 152,\n",
              " 'approximately': 153,\n",
              " 'hours': 154,\n",
              " 'per': 155,\n",
              " 'mode': 156,\n",
              " 'instruction': 157,\n",
              " 'hinglish': 158,\n",
              " 'mix': 159,\n",
              " 'hindi': 160,\n",
              " 'english': 161,\n",
              " 'informed': 162,\n",
              " 'upcoming': 163,\n",
              " 'an': 164,\n",
              " 'email': 165,\n",
              " 'whatsapp': 166,\n",
              " 'reminder': 167,\n",
              " 'before': 168,\n",
              " 'technical': 169,\n",
              " 'background': 170,\n",
              " 'beginner': 171,\n",
              " 'friendly': 172,\n",
              " 'possible': 173,\n",
              " 'middle': 174,\n",
              " 'absolutely': 175,\n",
              " 'once': 176,\n",
              " 'past': 177,\n",
              " 'unlocked': 178,\n",
              " 'solutions': 179,\n",
              " 'self': 180,\n",
              " 'evaluation': 181,\n",
              " 'build': 182,\n",
              " 'real': 183,\n",
              " 'projects': 184,\n",
              " 'module': 185,\n",
              " 'includes': 186,\n",
              " 'capstone': 187,\n",
              " 'project': 188,\n",
              " 'contact': 189,\n",
              " 'general': 190,\n",
              " 'questions': 191,\n",
              " 'my': 192,\n",
              " 'must': 193,\n",
              " 'done': 194,\n",
              " 'our': 195,\n",
              " 'website': 196,\n",
              " 'one': 197,\n",
              " 'go': 198,\n",
              " 'validity': 199,\n",
              " '30': 200,\n",
              " 'days': 201,\n",
              " 'date': 202,\n",
              " 'there': 203,\n",
              " 'outside': 204,\n",
              " 'india': 205,\n",
              " 'please': 206,\n",
              " 'us': 207,\n",
              " 'international': 208,\n",
              " 'options': 209,\n",
              " 'post': 210,\n",
              " 'queries': 211,\n",
              " 'long': 212,\n",
              " 'paid': 213,\n",
              " 'content': 214,\n",
              " 'active': 215,\n",
              " 'after': 216,\n",
              " 'completing': 217,\n",
              " 'extended': 218,\n",
              " 'dec': 219,\n",
              " '2025': 220,\n",
              " 'why': 221,\n",
              " 'due': 222,\n",
              " 'low': 223,\n",
              " 'clear': 224,\n",
              " 'available': 225,\n",
              " 'ask': 226,\n",
              " 'earlier': 227,\n",
              " 'late': 228,\n",
              " 'just': 229,\n",
              " 'select': 230,\n",
              " 'correct': 231,\n",
              " 'week': 232,\n",
              " 'criteria': 233,\n",
              " 'complete': 234,\n",
              " 'attempt': 235,\n",
              " 'least': 236,\n",
              " '80': 237,\n",
              " 'include': 238,\n",
              " 'portfolio': 239,\n",
              " 'building': 240,\n",
              " 'mock': 241,\n",
              " 'resume': 242,\n",
              " 'review': 243,\n",
              " 'referral': 244,\n",
              " 'network': 245,\n",
              " 'hunting': 246,\n",
              " 'strategy': 247,\n",
              " 'interview': 248,\n",
              " 'placements': 249,\n",
              " 'assurance': 250}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in data.split('\\n'):\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErsDLvRTqIfG",
        "outputId": "27861e19-4da5-46ed-8911-6faa3e9c5f5b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " About the Program\n",
            "What is the fee for the Full Stack Web Development Bootcamp?\n",
            "The bootcamp follows a monthly subscription model where you pay Rs 999/month.\n",
            "What is the total duration of the bootcamp?\n",
            "The total duration of the bootcamp is 6 months, so the total fee becomes 999*6 = Rs 6000 (approx).\n",
            "What is the syllabus of the bootcamp?\n",
            "We cover the following modules:\n",
            "HTML & CSS\n",
            "JavaScript (Basic to Advanced)\n",
            "Version Control with Git & GitHub\n",
            "React.js\n",
            "Node.js & Express\n",
            "MongoDB & SQL\n",
            "REST APIs\n",
            "Authentication & Authorization\n",
            "Deployment using Vercel & AWS\n",
            "You can check the detailed syllabus here - https://example.com/fullstack-syllabus\n",
            "Will DevOps or CI/CD be covered in this bootcamp?\n",
            "Only introductory topics will be covered; the main focus is on MERN stack development.\n",
            "What if I miss a live session? Will I get a recording?\n",
            "Yes, all sessions are recorded and uploaded to your dashboard.\n",
            "Where can I find the class schedule?\n",
            "Check the official schedule sheet here - https://example.com/schedule\n",
            "What is the average duration of a live session?\n",
            "Approximately 2.5 hours per session.\n",
            "What is the mode of instruction?\n",
            "Hinglish (mix of Hindi and English)\n",
            "How will I be informed about the upcoming class?\n",
            "You will get an email and WhatsApp reminder before each session.\n",
            "Can I join the course if I have no technical background?\n",
            "Yes, the bootcamp is beginner-friendly.\n",
            "Is it possible to join in the middle of the bootcamp?\n",
            "Absolutely. Once you join, all past sessions will be unlocked.\n",
            "Do I need to submit assignments?\n",
            "No need to submit. Solutions will be provided for self-evaluation.\n",
            "Will we build real projects in the course?\n",
            "Yes, each module includes a capstone project.\n",
            "Where can I contact for general support?\n",
            "You can write to support@webcamp.in\n",
            "Payment/Registration Questions\n",
            "Where should I make my payments?\n",
            "All payments must be done on our official website - https://example.com\n",
            "Can I pay the full fee in one go?\n",
            "No, the bootcamp follows a monthly subscription model only.\n",
            "What is the validity of each monthly payment?\n",
            "30 days from the date of payment.\n",
            "Is there a refund policy?\n",
            "Yes, we offer a 5-day refund policy from the day of payment.\n",
            "I live outside India and can’t make a payment, what should I do?\n",
            "Please write to us at support@webcamp.in for international payment options.\n",
            "Post Registration Queries\n",
            "How long will I have access to paid content?\n",
            "Till your subscription is active. After completing all 6 payments, access is extended till Dec 2025.\n",
            "Why is lifetime access not provided?\n",
            "Due to the low course fee, we can’t offer lifetime access.\n",
            "How can I clear doubts?\n",
            "Submit the doubt form available in your dashboard to schedule a 1-on-1 session.\n",
            "Can I ask doubts from earlier sessions if I join late?\n",
            "Yes, just select the correct session/week in the doubt form.\n",
            "Certificate & Placement Assistance\n",
            "What is the criteria to get the certificate?\n",
            "1. Complete payment (Rs 6000 total)\n",
            "2. Attempt at least 80% of the assignments\n",
            "What does placement assistance include?\n",
            "Portfolio building\n",
            "Mock interviews\n",
            "Resume review\n",
            "Referral network access\n",
            "Job hunting strategy sessions\n",
            "Does it guarantee a job or interview?\n",
            "No, we do not guarantee placements or interviews. Assistance is provided, not assurance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = []\n",
        "\n",
        "for sentence in data.split('\\n'):\n",
        "  tokanized_sentence = tokanizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1, len(tokanized_sentence)):\n",
        "    n_gram_seq = tokanized_sentence[:i+1]\n",
        "    input_seq.append(n_gram_seq)\n"
      ],
      "metadata": {
        "id": "wsXHZb5taGZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmXPMN2rbCLx",
        "outputId": "507aac5a-aa98-446c-f91b-27d5c038d24e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[56, 1],\n",
              " [56, 1, 101],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 19],\n",
              " [7, 2, 1, 19, 20],\n",
              " [7, 2, 1, 19, 20, 1],\n",
              " [7, 2, 1, 19, 20, 1, 57],\n",
              " [7, 2, 1, 19, 20, 1, 57, 58],\n",
              " [7, 2, 1, 19, 20, 1, 57, 58, 102],\n",
              " [7, 2, 1, 19, 20, 1, 57, 58, 102, 59],\n",
              " [7, 2, 1, 19, 20, 1, 57, 58, 102, 59, 8],\n",
              " [1, 8],\n",
              " [1, 8, 60],\n",
              " [1, 8, 60, 4],\n",
              " [1, 8, 60, 4, 29],\n",
              " [1, 8, 60, 4, 29, 30],\n",
              " [1, 8, 60, 4, 29, 30, 61],\n",
              " [1, 8, 60, 4, 29, 30, 61, 21],\n",
              " [1, 8, 60, 4, 29, 30, 61, 21, 15],\n",
              " [1, 8, 60, 4, 29, 30, 61, 21, 15, 62],\n",
              " [1, 8, 60, 4, 29, 30, 61, 21, 15, 62, 31],\n",
              " [1, 8, 60, 4, 29, 30, 61, 21, 15, 62, 31, 63],\n",
              " [1, 8, 60, 4, 29, 30, 61, 21, 15, 62, 31, 63, 103],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 22],\n",
              " [7, 2, 1, 22, 32],\n",
              " [7, 2, 1, 22, 32, 5],\n",
              " [7, 2, 1, 22, 32, 5, 1],\n",
              " [7, 2, 1, 22, 32, 5, 1, 8],\n",
              " [1, 22],\n",
              " [1, 22, 32],\n",
              " [1, 22, 32, 5],\n",
              " [1, 22, 32, 5, 1],\n",
              " [1, 22, 32, 5, 1, 8],\n",
              " [1, 22, 32, 5, 1, 8, 2],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19, 106],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19, 106, 63],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19, 106, 63, 33],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19, 106, 63, 33, 31],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19, 106, 63, 33, 31, 64],\n",
              " [1, 22, 32, 5, 1, 8, 2, 33, 104, 105, 1, 22, 19, 106, 63, 33, 31, 64, 107],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 34],\n",
              " [7, 2, 1, 34, 5],\n",
              " [7, 2, 1, 34, 5, 1],\n",
              " [7, 2, 1, 34, 5, 1, 8],\n",
              " [16, 108],\n",
              " [16, 108, 1],\n",
              " [16, 108, 1, 109],\n",
              " [16, 108, 1, 109, 110],\n",
              " [111, 112],\n",
              " [113, 114],\n",
              " [113, 114, 6],\n",
              " [113, 114, 6, 115],\n",
              " [116, 117],\n",
              " [116, 117, 118],\n",
              " [116, 117, 118, 119],\n",
              " [116, 117, 118, 119, 120],\n",
              " [121, 65],\n",
              " [122, 65],\n",
              " [122, 65, 123],\n",
              " [124, 125],\n",
              " [126, 127],\n",
              " [128, 129],\n",
              " [130, 131],\n",
              " [130, 131, 132],\n",
              " [130, 131, 132, 133],\n",
              " [15, 10],\n",
              " [15, 10, 66],\n",
              " [15, 10, 66, 1],\n",
              " [15, 10, 66, 1, 134],\n",
              " [15, 10, 66, 1, 134, 34],\n",
              " [15, 10, 66, 1, 134, 34, 67],\n",
              " [15, 10, 66, 1, 134, 34, 67, 35],\n",
              " [15, 10, 66, 1, 134, 34, 67, 35, 36],\n",
              " [15, 10, 66, 1, 134, 34, 67, 35, 36, 37],\n",
              " [15, 10, 66, 1, 134, 34, 67, 35, 36, 37, 135],\n",
              " [15, 10, 66, 1, 134, 34, 67, 35, 36, 37, 135, 34],\n",
              " [9, 136],\n",
              " [9, 136, 38],\n",
              " [9, 136, 38, 137],\n",
              " [9, 136, 38, 137, 138],\n",
              " [9, 136, 38, 137, 138, 13],\n",
              " [9, 136, 38, 137, 138, 13, 68],\n",
              " [9, 136, 38, 137, 138, 13, 68, 11],\n",
              " [9, 136, 38, 137, 138, 13, 68, 11, 139],\n",
              " [9, 136, 38, 137, 138, 13, 68, 11, 139, 8],\n",
              " [69, 140],\n",
              " [69, 140, 141],\n",
              " [69, 140, 141, 9],\n",
              " [69, 140, 141, 9, 13],\n",
              " [69, 140, 141, 9, 13, 68],\n",
              " [69, 140, 141, 9, 13, 68, 1],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142, 143],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142, 143, 2],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142, 143, 2, 39],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142, 143, 2, 39, 144],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142, 143, 2, 39, 144, 58],\n",
              " [69, 140, 141, 9, 13, 68, 1, 142, 143, 2, 39, 144, 58, 59],\n",
              " [7, 40],\n",
              " [7, 40, 3],\n",
              " [7, 40, 3, 145],\n",
              " [7, 40, 3, 145, 4],\n",
              " [7, 40, 3, 145, 4, 41],\n",
              " [7, 40, 3, 145, 4, 41, 14],\n",
              " [7, 40, 3, 145, 4, 41, 14, 9],\n",
              " [7, 40, 3, 145, 4, 41, 14, 9, 3],\n",
              " [7, 40, 3, 145, 4, 41, 14, 9, 3, 42],\n",
              " [7, 40, 3, 145, 4, 41, 14, 9, 3, 42, 4],\n",
              " [7, 40, 3, 145, 4, 41, 14, 9, 3, 42, 4, 146],\n",
              " [17, 23],\n",
              " [17, 23, 24],\n",
              " [17, 23, 24, 147],\n",
              " [17, 23, 24, 147, 148],\n",
              " [17, 23, 24, 147, 148, 25],\n",
              " [17, 23, 24, 147, 148, 25, 149],\n",
              " [17, 23, 24, 147, 148, 25, 149, 6],\n",
              " [17, 23, 24, 147, 148, 25, 149, 6, 43],\n",
              " [17, 23, 24, 147, 148, 25, 149, 6, 43, 70],\n",
              " [21, 10],\n",
              " [21, 10, 3],\n",
              " [21, 10, 3, 150],\n",
              " [21, 10, 3, 150, 1],\n",
              " [21, 10, 3, 150, 1, 71],\n",
              " [21, 10, 3, 150, 1, 71, 26],\n",
              " [66, 1],\n",
              " [66, 1, 72],\n",
              " [66, 1, 72, 26],\n",
              " [66, 1, 72, 26, 151],\n",
              " [66, 1, 72, 26, 151, 67],\n",
              " [66, 1, 72, 26, 151, 67, 35],\n",
              " [66, 1, 72, 26, 151, 67, 35, 36],\n",
              " [66, 1, 72, 26, 151, 67, 35, 36, 37],\n",
              " [66, 1, 72, 26, 151, 67, 35, 36, 37, 26],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 152],\n",
              " [7, 2, 1, 152, 32],\n",
              " [7, 2, 1, 152, 32, 5],\n",
              " [7, 2, 1, 152, 32, 5, 4],\n",
              " [7, 2, 1, 152, 32, 5, 4, 41],\n",
              " [7, 2, 1, 152, 32, 5, 4, 41, 14],\n",
              " [153, 73],\n",
              " [153, 73, 74],\n",
              " [153, 73, 74, 154],\n",
              " [153, 73, 74, 154, 155],\n",
              " [153, 73, 74, 154, 155, 14],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 156],\n",
              " [7, 2, 1, 156, 5],\n",
              " [7, 2, 1, 156, 5, 157],\n",
              " [158, 159],\n",
              " [158, 159, 5],\n",
              " [158, 159, 5, 160],\n",
              " [158, 159, 5, 160, 25],\n",
              " [158, 159, 5, 160, 25, 161],\n",
              " [44, 9],\n",
              " [44, 9, 3],\n",
              " [44, 9, 3, 13],\n",
              " [44, 9, 3, 13, 162],\n",
              " [44, 9, 3, 13, 162, 56],\n",
              " [44, 9, 3, 13, 162, 56, 1],\n",
              " [44, 9, 3, 13, 162, 56, 1, 163],\n",
              " [44, 9, 3, 13, 162, 56, 1, 163, 71],\n",
              " [15, 9],\n",
              " [15, 9, 42],\n",
              " [15, 9, 42, 164],\n",
              " [15, 9, 42, 164, 165],\n",
              " [15, 9, 42, 164, 165, 25],\n",
              " [15, 9, 42, 164, 165, 25, 166],\n",
              " [15, 9, 42, 164, 165, 25, 166, 167],\n",
              " [15, 9, 42, 164, 165, 25, 166, 167, 168],\n",
              " [15, 9, 42, 164, 165, 25, 166, 167, 168, 45],\n",
              " [15, 9, 42, 164, 165, 25, 166, 167, 168, 45, 14],\n",
              " [10, 3],\n",
              " [10, 3, 27],\n",
              " [10, 3, 27, 1],\n",
              " [10, 3, 27, 1, 46],\n",
              " [10, 3, 27, 1, 46, 40],\n",
              " [10, 3, 27, 1, 46, 40, 3],\n",
              " [10, 3, 27, 1, 46, 40, 3, 75],\n",
              " [10, 3, 27, 1, 46, 40, 3, 75, 28],\n",
              " [10, 3, 27, 1, 46, 40, 3, 75, 28, 169],\n",
              " [10, 3, 27, 1, 46, 40, 3, 75, 28, 169, 170],\n",
              " [17, 1],\n",
              " [17, 1, 8],\n",
              " [17, 1, 8, 2],\n",
              " [17, 1, 8, 2, 171],\n",
              " [17, 1, 8, 2, 171, 172],\n",
              " [2, 76],\n",
              " [2, 76, 173],\n",
              " [2, 76, 173, 6],\n",
              " [2, 76, 173, 6, 27],\n",
              " [2, 76, 173, 6, 27, 11],\n",
              " [2, 76, 173, 6, 27, 11, 1],\n",
              " [2, 76, 173, 6, 27, 11, 1, 174],\n",
              " [2, 76, 173, 6, 27, 11, 1, 174, 5],\n",
              " [2, 76, 173, 6, 27, 11, 1, 174, 5, 1],\n",
              " [2, 76, 173, 6, 27, 11, 1, 174, 5, 1, 8],\n",
              " [175, 176],\n",
              " [175, 176, 15],\n",
              " [175, 176, 15, 27],\n",
              " [175, 176, 15, 27, 23],\n",
              " [175, 176, 15, 27, 23, 177],\n",
              " [175, 176, 15, 27, 23, 177, 24],\n",
              " [175, 176, 15, 27, 23, 177, 24, 9],\n",
              " [175, 176, 15, 27, 23, 177, 24, 9, 13],\n",
              " [175, 176, 15, 27, 23, 177, 24, 9, 13, 178],\n",
              " [47, 3],\n",
              " [47, 3, 77],\n",
              " [47, 3, 77, 6],\n",
              " [47, 3, 77, 6, 48],\n",
              " [47, 3, 77, 6, 48, 78],\n",
              " [28, 77],\n",
              " [28, 77, 6],\n",
              " [28, 77, 6, 48],\n",
              " [28, 77, 6, 48, 179],\n",
              " [28, 77, 6, 48, 179, 9],\n",
              " [28, 77, 6, 48, 179, 9, 13],\n",
              " [28, 77, 6, 48, 179, 9, 13, 49],\n",
              " [28, 77, 6, 48, 179, 9, 13, 49, 20],\n",
              " [28, 77, 6, 48, 179, 9, 13, 49, 20, 180],\n",
              " [28, 77, 6, 48, 179, 9, 13, 49, 20, 180, 181],\n",
              " [9, 16],\n",
              " [9, 16, 182],\n",
              " [9, 16, 182, 183],\n",
              " [9, 16, 182, 183, 184],\n",
              " [9, 16, 182, 183, 184, 11],\n",
              " [9, 16, 182, 183, 184, 11, 1],\n",
              " [9, 16, 182, 183, 184, 11, 1, 46],\n",
              " [17, 45],\n",
              " [17, 45, 185],\n",
              " [17, 45, 185, 186],\n",
              " [17, 45, 185, 186, 4],\n",
              " [17, 45, 185, 186, 4, 187],\n",
              " [17, 45, 185, 186, 4, 187, 188],\n",
              " [21, 10],\n",
              " [21, 10, 3],\n",
              " [21, 10, 3, 189],\n",
              " [21, 10, 3, 189, 20],\n",
              " [21, 10, 3, 189, 20, 190],\n",
              " [21, 10, 3, 189, 20, 190, 50],\n",
              " [15, 10],\n",
              " [15, 10, 79],\n",
              " [15, 10, 79, 6],\n",
              " [15, 10, 79, 6, 50],\n",
              " [15, 10, 79, 6, 50, 80],\n",
              " [15, 10, 79, 6, 50, 80, 11],\n",
              " [12, 81],\n",
              " [12, 81, 191],\n",
              " [21, 82],\n",
              " [21, 82, 3],\n",
              " [21, 82, 3, 83],\n",
              " [21, 82, 3, 83, 192],\n",
              " [21, 82, 3, 83, 192, 51],\n",
              " [23, 51],\n",
              " [23, 51, 193],\n",
              " [23, 51, 193, 13],\n",
              " [23, 51, 193, 13, 194],\n",
              " [23, 51, 193, 13, 194, 39],\n",
              " [23, 51, 193, 13, 194, 39, 195],\n",
              " [23, 51, 193, 13, 194, 39, 195, 72],\n",
              " [23, 51, 193, 13, 194, 39, 195, 72, 196],\n",
              " [23, 51, 193, 13, 194, 39, 195, 72, 196, 35],\n",
              " [23, 51, 193, 13, 194, 39, 195, 72, 196, 35, 36],\n",
              " [23, 51, 193, 13, 194, 39, 195, 72, 196, 35, 36, 37],\n",
              " [10, 3],\n",
              " [10, 3, 62],\n",
              " [10, 3, 62, 1],\n",
              " [10, 3, 62, 1, 57],\n",
              " [10, 3, 62, 1, 57, 19],\n",
              " [10, 3, 62, 1, 57, 19, 11],\n",
              " [10, 3, 62, 1, 57, 19, 11, 197],\n",
              " [10, 3, 62, 1, 57, 19, 11, 197, 198],\n",
              " [28, 1],\n",
              " [28, 1, 8],\n",
              " [28, 1, 8, 60],\n",
              " [28, 1, 8, 60, 4],\n",
              " [28, 1, 8, 60, 4, 29],\n",
              " [28, 1, 8, 60, 4, 29, 30],\n",
              " [28, 1, 8, 60, 4, 29, 30, 61],\n",
              " [28, 1, 8, 60, 4, 29, 30, 61, 69],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 199],\n",
              " [7, 2, 1, 199, 5],\n",
              " [7, 2, 1, 199, 5, 45],\n",
              " [7, 2, 1, 199, 5, 45, 29],\n",
              " [7, 2, 1, 199, 5, 45, 29, 12],\n",
              " [200, 201],\n",
              " [200, 201, 52],\n",
              " [200, 201, 52, 1],\n",
              " [200, 201, 52, 1, 202],\n",
              " [200, 201, 52, 1, 202, 5],\n",
              " [200, 201, 52, 1, 202, 5, 12],\n",
              " [2, 203],\n",
              " [2, 203, 4],\n",
              " [2, 203, 4, 84],\n",
              " [2, 203, 4, 84, 85],\n",
              " [17, 16],\n",
              " [17, 16, 86],\n",
              " [17, 16, 86, 4],\n",
              " [17, 16, 86, 4, 74],\n",
              " [17, 16, 86, 4, 74, 87],\n",
              " [17, 16, 86, 4, 74, 87, 84],\n",
              " [17, 16, 86, 4, 74, 87, 84, 85],\n",
              " [17, 16, 86, 4, 74, 87, 84, 85, 52],\n",
              " [17, 16, 86, 4, 74, 87, 84, 85, 52, 1],\n",
              " [17, 16, 86, 4, 74, 87, 84, 85, 52, 1, 87],\n",
              " [17, 16, 86, 4, 74, 87, 84, 85, 52, 1, 87, 5],\n",
              " [17, 16, 86, 4, 74, 87, 84, 85, 52, 1, 87, 5, 12],\n",
              " [3, 41],\n",
              " [3, 41, 204],\n",
              " [3, 41, 204, 205],\n",
              " [3, 41, 204, 205, 25],\n",
              " [3, 41, 204, 205, 25, 88],\n",
              " [3, 41, 204, 205, 25, 88, 83],\n",
              " [3, 41, 204, 205, 25, 88, 83, 4],\n",
              " [3, 41, 204, 205, 25, 88, 83, 4, 12],\n",
              " [3, 41, 204, 205, 25, 88, 83, 4, 12, 7],\n",
              " [3, 41, 204, 205, 25, 88, 83, 4, 12, 7, 82],\n",
              " [3, 41, 204, 205, 25, 88, 83, 4, 12, 7, 82, 3],\n",
              " [3, 41, 204, 205, 25, 88, 83, 4, 12, 7, 82, 3, 47],\n",
              " [206, 79],\n",
              " [206, 79, 6],\n",
              " [206, 79, 6, 207],\n",
              " [206, 79, 6, 207, 89],\n",
              " [206, 79, 6, 207, 89, 50],\n",
              " [206, 79, 6, 207, 89, 50, 80],\n",
              " [206, 79, 6, 207, 89, 50, 80, 11],\n",
              " [206, 79, 6, 207, 89, 50, 80, 11, 20],\n",
              " [206, 79, 6, 207, 89, 50, 80, 11, 20, 208],\n",
              " [206, 79, 6, 207, 89, 50, 80, 11, 20, 208, 12],\n",
              " [206, 79, 6, 207, 89, 50, 80, 11, 20, 208, 12, 209],\n",
              " [210, 81],\n",
              " [210, 81, 211],\n",
              " [44, 212],\n",
              " [44, 212, 9],\n",
              " [44, 212, 9, 3],\n",
              " [44, 212, 9, 3, 75],\n",
              " [44, 212, 9, 3, 75, 18],\n",
              " [44, 212, 9, 3, 75, 18, 6],\n",
              " [44, 212, 9, 3, 75, 18, 6, 213],\n",
              " [44, 212, 9, 3, 75, 18, 6, 213, 214],\n",
              " [90, 43],\n",
              " [90, 43, 30],\n",
              " [90, 43, 30, 2],\n",
              " [90, 43, 30, 2, 215],\n",
              " [90, 43, 30, 2, 215, 216],\n",
              " [90, 43, 30, 2, 215, 216, 217],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51, 18],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51, 18, 2],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51, 18, 2, 218],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51, 18, 2, 218, 90],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51, 18, 2, 218, 90, 219],\n",
              " [90, 43, 30, 2, 215, 216, 217, 23, 33, 51, 18, 2, 218, 90, 219, 220],\n",
              " [221, 2],\n",
              " [221, 2, 91],\n",
              " [221, 2, 91, 18],\n",
              " [221, 2, 91, 18, 53],\n",
              " [221, 2, 91, 18, 53, 49],\n",
              " [222, 6],\n",
              " [222, 6, 1],\n",
              " [222, 6, 1, 223],\n",
              " [222, 6, 1, 223, 46],\n",
              " [222, 6, 1, 223, 46, 19],\n",
              " [222, 6, 1, 223, 46, 19, 16],\n",
              " [222, 6, 1, 223, 46, 19, 16, 88],\n",
              " [222, 6, 1, 223, 46, 19, 16, 88, 86],\n",
              " [222, 6, 1, 223, 46, 19, 16, 88, 86, 91],\n",
              " [222, 6, 1, 223, 46, 19, 16, 88, 86, 91, 18],\n",
              " [44, 10],\n",
              " [44, 10, 3],\n",
              " [44, 10, 3, 224],\n",
              " [44, 10, 3, 224, 92],\n",
              " [48, 1],\n",
              " [48, 1, 93],\n",
              " [48, 1, 93, 94],\n",
              " [48, 1, 93, 94, 225],\n",
              " [48, 1, 93, 94, 225, 11],\n",
              " [48, 1, 93, 94, 225, 11, 43],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6, 26],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6, 26, 4],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6, 26, 4, 54],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6, 26, 4, 54, 39],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6, 26, 4, 54, 39, 54],\n",
              " [48, 1, 93, 94, 225, 11, 43, 70, 6, 26, 4, 54, 39, 54, 14],\n",
              " [10, 3],\n",
              " [10, 3, 226],\n",
              " [10, 3, 226, 92],\n",
              " [10, 3, 226, 92, 52],\n",
              " [10, 3, 226, 92, 52, 227],\n",
              " [10, 3, 226, 92, 52, 227, 24],\n",
              " [10, 3, 226, 92, 52, 227, 24, 40],\n",
              " [10, 3, 226, 92, 52, 227, 24, 40, 3],\n",
              " [10, 3, 226, 92, 52, 227, 24, 40, 3, 27],\n",
              " [10, 3, 226, 92, 52, 227, 24, 40, 3, 27, 228],\n",
              " [17, 229],\n",
              " [17, 229, 230],\n",
              " [17, 229, 230, 1],\n",
              " [17, 229, 230, 1, 231],\n",
              " [17, 229, 230, 1, 231, 14],\n",
              " [17, 229, 230, 1, 231, 14, 232],\n",
              " [17, 229, 230, 1, 231, 14, 232, 11],\n",
              " [17, 229, 230, 1, 231, 14, 232, 11, 1],\n",
              " [17, 229, 230, 1, 231, 14, 232, 11, 1, 93],\n",
              " [17, 229, 230, 1, 231, 14, 232, 11, 1, 93, 94],\n",
              " [95, 96],\n",
              " [95, 96, 55],\n",
              " [7, 2],\n",
              " [7, 2, 1],\n",
              " [7, 2, 1, 233],\n",
              " [7, 2, 1, 233, 6],\n",
              " [7, 2, 1, 233, 6, 42],\n",
              " [7, 2, 1, 233, 6, 42, 1],\n",
              " [7, 2, 1, 233, 6, 42, 1, 95],\n",
              " [54, 234],\n",
              " [54, 234, 12],\n",
              " [54, 234, 12, 31],\n",
              " [54, 234, 12, 31, 64],\n",
              " [54, 234, 12, 31, 64, 22],\n",
              " [73, 235],\n",
              " [73, 235, 89],\n",
              " [73, 235, 89, 236],\n",
              " [73, 235, 89, 236, 237],\n",
              " [73, 235, 89, 236, 237, 5],\n",
              " [73, 235, 89, 236, 237, 5, 1],\n",
              " [73, 235, 89, 236, 237, 5, 1, 78],\n",
              " [7, 97],\n",
              " [7, 97, 96],\n",
              " [7, 97, 96, 55],\n",
              " [7, 97, 96, 55, 238],\n",
              " [239, 240],\n",
              " [241, 98],\n",
              " [242, 243],\n",
              " [244, 245],\n",
              " [244, 245, 18],\n",
              " [99, 246],\n",
              " [99, 246, 247],\n",
              " [99, 246, 247, 24],\n",
              " [97, 76],\n",
              " [97, 76, 100],\n",
              " [97, 76, 100, 4],\n",
              " [97, 76, 100, 4, 99],\n",
              " [97, 76, 100, 4, 99, 38],\n",
              " [97, 76, 100, 4, 99, 38, 248],\n",
              " [28, 16],\n",
              " [28, 16, 47],\n",
              " [28, 16, 47, 53],\n",
              " [28, 16, 47, 53, 100],\n",
              " [28, 16, 47, 53, 100, 249],\n",
              " [28, 16, 47, 53, 100, 249, 38],\n",
              " [28, 16, 47, 53, 100, 249, 38, 98],\n",
              " [28, 16, 47, 53, 100, 249, 38, 98, 55],\n",
              " [28, 16, 47, 53, 100, 249, 38, 98, 55, 2],\n",
              " [28, 16, 47, 53, 100, 249, 38, 98, 55, 2, 49],\n",
              " [28, 16, 47, 53, 100, 249, 38, 98, 55, 2, 49, 53],\n",
              " [28, 16, 47, 53, 100, 249, 38, 98, 55, 2, 49, 53, 250]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(x) for x in input_seq])"
      ],
      "metadata": {
        "id": "o1dfdSbaeROO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZx3rp2mfCnq",
        "outputId": "54fdd5b1-a388-4f71-a748-658336b1c923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "61f3cc05",
        "outputId": "46919fab-e614-41c6-9376-72193efedffb"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_input_seq = pad_sequences(input_seq, maxlen=max_length, padding='pre')\n",
        "display(padded_input_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  56,   1],\n",
              "       [  0,   0,   0, ...,  56,   1, 101],\n",
              "       [  0,   0,   0, ...,   0,   7,   2],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  55,   2,  49],\n",
              "       [  0,   0,   0, ...,   2,  49,  53],\n",
              "       [  0,   0,   0, ...,  49,  53, 250]], dtype=int32)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_seq[:, :-1]\n",
        "y = padded_input_seq[:, -1]"
      ],
      "metadata": {
        "id": "Fn_N1zL1fdFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdkiuZpffy8z",
        "outputId": "98b58f52-afb2-4f76-e15f-64a73b62bdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0 ...  0  0 56]\n",
            " [ 0  0  0 ...  0 56  1]\n",
            " [ 0  0  0 ...  0  0  7]\n",
            " ...\n",
            " [ 0  0  0 ... 98 55  2]\n",
            " [ 0  0  0 ... 55  2 49]\n",
            " [ 0  0  0 ...  2 49 53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkG4oPdjf0GL",
        "outputId": "8b1193a3-8981-4378-fa44-651d0f6251dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1 101   2   1  19  20   1  57  58 102  59   8   8  60   4  29  30  61\n",
            "  21  15  62  31  63 103   2   1  22  32   5   1   8  22  32   5   1   8\n",
            "   2  33 104 105   1  22  19 106  63  33  31  64 107   2   1  34   5   1\n",
            "   8 108   1 109 110 112 114   6 115 117 118 119 120  65  65 123 125 127\n",
            " 129 131 132 133  10  66   1 134  34  67  35  36  37 135  34 136  38 137\n",
            " 138  13  68  11 139   8 140 141   9  13  68   1 142 143   2  39 144  58\n",
            "  59  40   3 145   4  41  14   9   3  42   4 146  23  24 147 148  25 149\n",
            "   6  43  70  10   3 150   1  71  26   1  72  26 151  67  35  36  37  26\n",
            "   2   1 152  32   5   4  41  14  73  74 154 155  14   2   1 156   5 157\n",
            " 159   5 160  25 161   9   3  13 162  56   1 163  71   9  42 164 165  25\n",
            " 166 167 168  45  14   3  27   1  46  40   3  75  28 169 170   1   8   2\n",
            " 171 172  76 173   6  27  11   1 174   5   1   8 176  15  27  23 177  24\n",
            "   9  13 178   3  77   6  48  78  77   6  48 179   9  13  49  20 180 181\n",
            "  16 182 183 184  11   1  46  45 185 186   4 187 188  10   3 189  20 190\n",
            "  50  10  79   6  50  80  11  81 191  82   3  83 192  51  51 193  13 194\n",
            "  39 195  72 196  35  36  37   3  62   1  57  19  11 197 198   1   8  60\n",
            "   4  29  30  61  69   2   1 199   5  45  29  12 201  52   1 202   5  12\n",
            " 203   4  84  85  16  86   4  74  87  84  85  52   1  87   5  12  41 204\n",
            " 205  25  88  83   4  12   7  82   3  47  79   6 207  89  50  80  11  20\n",
            " 208  12 209  81 211 212   9   3  75  18   6 213 214  43  30   2 215 216\n",
            " 217  23  33  51  18   2 218  90 219 220   2  91  18  53  49   6   1 223\n",
            "  46  19  16  88  86  91  18  10   3 224  92   1  93  94 225  11  43  70\n",
            "   6  26   4  54  39  54  14   3 226  92  52 227  24  40   3  27 228 229\n",
            " 230   1 231  14 232  11   1  93  94  96  55   2   1 233   6  42   1  95\n",
            " 234  12  31  64  22 235  89 236 237   5   1  78  97  96  55 238 240  98\n",
            " 243 245  18 246 247  24  76 100   4  99  38 248  16  47  53 100 249  38\n",
            "  98  55   2  49  53 250]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO5OcLnXf3Qc",
        "outputId": "3bde5ac5-fa38-49b0-9ba6-737aa324179f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(474, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSbCv4Hdg81X",
        "outputId": "6a7dec68-d60a-4431-9bea-e134032d7fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(474,)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokanizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh4zWQtug9tV",
        "outputId": "bca5a844-617e-47ab-84d9-74bb196573cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=len(tokanizer.word_index)+1)"
      ],
      "metadata": {
        "id": "537fSDAShmCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivcs4tbriCuN",
        "outputId": "29f74e59-b021-473b-8a9f-50b25eec083b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(474, 251)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj14INVMiKHD",
        "outputId": "760c5134-a60c-4c09-bd92-cb8028506264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "SY1_VQYviOXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(251, 100, input_length=19))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(251, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYF1Tt1nna-n",
        "outputId": "329cf98c-69a1-4db3-9c27-a25e05ec280d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LAn95N3znpR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "yV1oudc9nstB",
        "outputId": "765fe144-b7e3-48c6-f10d-520d2415a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XV6y0n9nvRY",
        "outputId": "a26eda69-5833-45d2-bd81-9693f30ac307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.0445 - loss: 5.5113\n",
            "Epoch 2/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0724 - loss: 5.1857\n",
            "Epoch 3/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0839 - loss: 5.0107\n",
            "Epoch 4/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0832 - loss: 4.9568\n",
            "Epoch 5/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0680 - loss: 4.9901\n",
            "Epoch 6/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0801 - loss: 4.9032\n",
            "Epoch 7/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0817 - loss: 4.7929\n",
            "Epoch 8/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0917 - loss: 4.6941\n",
            "Epoch 9/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1129 - loss: 4.5675\n",
            "Epoch 10/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0951 - loss: 4.5790\n",
            "Epoch 11/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0963 - loss: 4.4217\n",
            "Epoch 12/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0916 - loss: 4.3826\n",
            "Epoch 13/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.1517 - loss: 4.1318\n",
            "Epoch 14/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1765 - loss: 3.9842\n",
            "Epoch 15/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1962 - loss: 3.8632\n",
            "Epoch 16/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2210 - loss: 3.6659\n",
            "Epoch 17/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2328 - loss: 3.5384\n",
            "Epoch 18/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2785 - loss: 3.3580\n",
            "Epoch 19/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2673 - loss: 3.2822\n",
            "Epoch 20/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3248 - loss: 3.0865\n",
            "Epoch 21/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3366 - loss: 2.9936\n",
            "Epoch 22/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3563 - loss: 2.8778\n",
            "Epoch 23/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.3787 - loss: 2.7613\n",
            "Epoch 24/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4628 - loss: 2.4856\n",
            "Epoch 25/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4886 - loss: 2.4256\n",
            "Epoch 26/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5215 - loss: 2.2637\n",
            "Epoch 27/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5372 - loss: 2.2519\n",
            "Epoch 28/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5583 - loss: 2.1343\n",
            "Epoch 29/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5897 - loss: 2.0097\n",
            "Epoch 30/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6042 - loss: 1.9441\n",
            "Epoch 31/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6184 - loss: 1.8586\n",
            "Epoch 32/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6923 - loss: 1.6561\n",
            "Epoch 33/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7280 - loss: 1.5560\n",
            "Epoch 34/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7277 - loss: 1.5135\n",
            "Epoch 35/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.7588 - loss: 1.4337\n",
            "Epoch 36/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.7789 - loss: 1.3810\n",
            "Epoch 37/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.8012 - loss: 1.3195\n",
            "Epoch 38/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8323 - loss: 1.2088\n",
            "Epoch 39/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8540 - loss: 1.1376\n",
            "Epoch 40/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8562 - loss: 1.0979\n",
            "Epoch 41/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8856 - loss: 1.0300\n",
            "Epoch 42/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9109 - loss: 0.9469\n",
            "Epoch 43/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8842 - loss: 0.9629\n",
            "Epoch 44/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9066 - loss: 0.8753\n",
            "Epoch 45/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9085 - loss: 0.8306\n",
            "Epoch 46/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9216 - loss: 0.7897\n",
            "Epoch 47/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8944 - loss: 0.7968\n",
            "Epoch 48/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9019 - loss: 0.7426\n",
            "Epoch 49/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9101 - loss: 0.6951\n",
            "Epoch 50/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9368 - loss: 0.6403\n",
            "Epoch 51/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9456 - loss: 0.6105\n",
            "Epoch 52/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9229 - loss: 0.5608\n",
            "Epoch 53/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9511 - loss: 0.5326\n",
            "Epoch 54/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9311 - loss: 0.5515\n",
            "Epoch 55/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9284 - loss: 0.5239\n",
            "Epoch 56/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9463 - loss: 0.4730\n",
            "Epoch 57/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9443 - loss: 0.4669\n",
            "Epoch 58/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9388 - loss: 0.4544\n",
            "Epoch 59/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9195 - loss: 0.4540\n",
            "Epoch 60/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9511 - loss: 0.3954\n",
            "Epoch 61/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9225 - loss: 0.4356\n",
            "Epoch 62/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9356 - loss: 0.4004\n",
            "Epoch 63/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9437 - loss: 0.3666\n",
            "Epoch 64/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9509 - loss: 0.3329\n",
            "Epoch 65/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9494 - loss: 0.3325\n",
            "Epoch 66/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9502 - loss: 0.3275\n",
            "Epoch 67/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9449 - loss: 0.3074\n",
            "Epoch 68/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9218 - loss: 0.3243\n",
            "Epoch 69/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9316 - loss: 0.3098\n",
            "Epoch 70/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9315 - loss: 0.3191\n",
            "Epoch 71/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9531 - loss: 0.2758\n",
            "Epoch 72/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9666 - loss: 0.2498\n",
            "Epoch 73/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9326 - loss: 0.2965\n",
            "Epoch 74/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9371 - loss: 0.2677\n",
            "Epoch 75/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9457 - loss: 0.2495\n",
            "Epoch 76/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9302 - loss: 0.2711\n",
            "Epoch 77/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9397 - loss: 0.2406\n",
            "Epoch 78/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9509 - loss: 0.2421\n",
            "Epoch 79/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9463 - loss: 0.2231\n",
            "Epoch 80/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9420 - loss: 0.2205\n",
            "Epoch 81/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9409 - loss: 0.2355\n",
            "Epoch 82/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9429 - loss: 0.2355\n",
            "Epoch 83/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9412 - loss: 0.2146\n",
            "Epoch 84/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9427 - loss: 0.2174\n",
            "Epoch 85/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9394 - loss: 0.2097\n",
            "Epoch 86/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9305 - loss: 0.2211\n",
            "Epoch 87/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9402 - loss: 0.2006\n",
            "Epoch 88/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9428 - loss: 0.2075\n",
            "Epoch 89/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9426 - loss: 0.2035\n",
            "Epoch 90/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9440 - loss: 0.1999\n",
            "Epoch 91/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9515 - loss: 0.1988\n",
            "Epoch 92/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9419 - loss: 0.1952\n",
            "Epoch 93/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9171 - loss: 0.2060\n",
            "Epoch 94/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9451 - loss: 0.1660\n",
            "Epoch 95/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9549 - loss: 0.1582\n",
            "Epoch 96/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9422 - loss: 0.1953\n",
            "Epoch 97/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9401 - loss: 0.1785\n",
            "Epoch 98/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9274 - loss: 0.1807\n",
            "Epoch 99/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9328 - loss: 0.1971\n",
            "Epoch 100/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9427 - loss: 0.1622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e53963429d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text = \"We cover\"\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokanizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokanizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKs6XrWspB1v",
        "outputId": "d68b50f1-7028-465f-f30d-f4b37b09f2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "We cover the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "We cover the following\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "We cover the following modules\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "We cover the following modules modules\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "We cover the following modules modules modules\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "We cover the following modules modules modules in\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "We cover the following modules modules modules in in\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "We cover the following modules modules modules in in the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "We cover the following modules modules modules in in the doubt\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "We cover the following modules modules modules in in the doubt form\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLxRxmpGp5iE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}